{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "26b77b61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/miguel.d.ferrusca/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/miguel.d.ferrusca/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "/Users/miguel.d.ferrusca/Documents/ironhack/lib/python3.9/site-packages/gensim/similarities/__init__.py:15: UserWarning: The gensim.similarities.levenshtein submodule is disabled, because the optional Levenshtein package <https://pypi.org/project/python-Levenshtein/> is unavailable. Install Levenhstein (e.g. `pip install python-Levenshtein`) to suppress this warning.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "#Library import\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import streamlit as st\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "from textblob import TextBlob\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "import spacy\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import time\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import requests\n",
    "from pathlib import Path\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bfb692a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scraper(asin, pages=10):\n",
    "    \n",
    "    df = pd.DataFrame(columns=['rating', 'content', 'title'])\n",
    "    pages = range(1,pages+1)\n",
    "    asin=asin\n",
    "    \n",
    "    for page in pages:\n",
    "        \n",
    "        print('Scraped {} page(s). Scraping page {}. Total reviews: {}'.format(page-1, page, len(df)))\n",
    "        \n",
    "        # iterable url\n",
    "        url = 'https://www.amazon.com/product-reviews/{}/ref=cm_cr_getr_d_paging_btm_prev_1?pageNumber={}'.format(asin, page)\n",
    "        \n",
    "        # getting soup\n",
    "        driver = webdriver.Chrome()\n",
    "        driver.get(url)\n",
    "        time.sleep(1)\n",
    "        page_source = driver.page_source\n",
    "        driver.quit()\n",
    "        soup = BeautifulSoup(page_source,'lxml')\n",
    "        \n",
    "        # parsing soup\n",
    "        reviews = soup.findAll(\"div\", {\"class\":\"a-section review aok-relative\"})\n",
    "        ## parsing reviews section\n",
    "        reviews = BeautifulSoup('<br/>'.join([str(tag) for tag in reviews]), 'html.parser')\n",
    "        \n",
    "        #getting title\n",
    "        title = soup.find(\"h1\", {\"class\":\"a-size-large a-text-ellipsis\"}).get_text()\n",
    "        \n",
    "        #getting content\n",
    "        contents = reviews.find_all(\"span\", {\"data-hook\":\"review-body\"})\n",
    "        content_lst = []\n",
    "        for content in contents:\n",
    "            text_ = content.find_all(\"span\")[0].get_text(\"\\n\").strip()\n",
    "            text_ = \". \".join(text_.splitlines())\n",
    "            text_ = re.sub(' +', ' ', text_)\n",
    "            content_lst.append(text_)\n",
    "            \n",
    "        #getting rating\n",
    "        ratings = reviews.find_all(\"i\", {\"data-hook\":\"review-star-rating\"})\n",
    "        full_rating_lst = []\n",
    "        for rating in ratings:\n",
    "            full_rating_lst.append(rating.find_all(\"span\")[0].contents[0])\n",
    "            \n",
    "        \n",
    "        rating_lst = []\n",
    "        for rating in full_rating_lst:\n",
    "            rating_lst.append(re.findall(\"\\d+\\.\\d+\", rating)[0])\n",
    "            \n",
    "            \n",
    "        # concatenating to main data frame\n",
    "        \n",
    "        temp_df = pd.DataFrame({'rating':rating_lst, 'content':content_lst, 'title':title})\n",
    "        df = df.append(temp_df)\n",
    "        \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "481fd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "asins = ['B00GAC1D2G', 'B07RZ74VLR', \n",
    "         'B01LZNGPY3', 'B01H6GUCCQ', \n",
    "         'B07KXQX3S3', 'B07VGRJDFY', \n",
    "         'B07SFKTLZM', 'B071JRMKBH', \n",
    "         'B01NAWKYZ0', 'B08FC6C75Y',\n",
    "         'B01LWVX2RG', 'B01N1037CV', \n",
    "         'B08WWFWRY6', 'B07GBZ4Q68', \n",
    "         'B01N5OKGLH', 'B08H9M7LDY', \n",
    "         'B07SL6ZXBL', 'B08DF248LD', \n",
    "         'B010KYDNDG', 'B00NLZUM36',\n",
    "         'B08CVB3Y8B', 'B07Y693ND1', \n",
    "         'B01MY7GHKJ', 'B088GH4X9W',\n",
    "         'B07F7T8J9P', 'B093B218BN',\n",
    "         'B07L7W915H', 'B07DHTN9LQ',\n",
    "         'B08XFPDXDR', 'B07WDGB9P5',\n",
    "         'B08G9J44ZN', 'B07VGRJDFY',\n",
    "         'B0931NN4PR', 'B092VT1JGD',\n",
    "         'B01N6QKT7H', 'B01N6S068R']\n",
    "\n",
    "asins = np.unique(asins).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbd317f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 19\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 19\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 19\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 10\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 20\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 30\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 40\n",
      "Scraped 0 page(s). Scraping page 1. Total reviews: 0\n",
      "Scraped 1 page(s). Scraping page 2. Total reviews: 0\n",
      "Scraped 2 page(s). Scraping page 3. Total reviews: 0\n",
      "Scraped 3 page(s). Scraping page 4. Total reviews: 0\n",
      "Scraped 4 page(s). Scraping page 5. Total reviews: 0\n"
     ]
    }
   ],
   "source": [
    "dataset = pd.DataFrame(columns=['rating', 'content', 'title'])\n",
    "\n",
    "for asin in asins:\n",
    "    dataset = dataset.append(scraper(asin,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b47a9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping products with high reviews\n",
    "\n",
    "reviews_by_product = dataset.groupby(dataset.title)['content'].agg('count').reset_index().sort_values(by='content', ascending=False)\n",
    "high_reviewed_prods = reviews_by_product[reviews_by_product['content']>=40]['title'].unique().tolist()\n",
    "\n",
    "clean_df = dataset[dataset['title'].isin(high_reviewed_prods)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ff738243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-6-364de5a1b6c4>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['polarity'] = [TextBlob(review).sentiment.polarity for review in clean_df['content'].tolist()]\n",
      "<ipython-input-6-364de5a1b6c4>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  clean_df['subjectivity'] = [TextBlob(review).sentiment.subjectivity for review in clean_df['content'].tolist()]\n"
     ]
    }
   ],
   "source": [
    "# Getting text polarity and subjectivity\n",
    "\n",
    "clean_df['polarity'] = [TextBlob(review).sentiment.polarity for review in clean_df['content'].tolist()]\n",
    "clean_df['subjectivity'] = [TextBlob(review).sentiment.subjectivity for review in clean_df['content'].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4ba33343",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exporting dataset\n",
    "\n",
    "path = 'data/'\n",
    "\n",
    "clean_df.to_csv(os.path.join(path,r'dataset.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef4057e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>polarity</th>\n",
       "      <th>subjectivity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I purchased two of these cards almost exactly ...</td>\n",
       "      <td>$10 PlayStation Store Gift Card [Digital Code]</td>\n",
       "      <td>-0.019792</td>\n",
       "      <td>0.230208</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5.0</td>\n",
       "      <td>Amazon wants to know what I think about this i...</td>\n",
       "      <td>$10 PlayStation Store Gift Card [Digital Code]</td>\n",
       "      <td>0.100676</td>\n",
       "      <td>0.358117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5.0</td>\n",
       "      <td>fast easy poor n broke due to fortnite i cant ...</td>\n",
       "      <td>$10 PlayStation Store Gift Card [Digital Code]</td>\n",
       "      <td>-0.003496</td>\n",
       "      <td>0.566414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.0</td>\n",
       "      <td>El tiempo de llegada del código es inmediato</td>\n",
       "      <td>$10 PlayStation Store Gift Card [Digital Code]</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2.0</td>\n",
       "      <td>This should be a professional service, if you'...</td>\n",
       "      <td>$10 PlayStation Store Gift Card [Digital Code]</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.462500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.0</td>\n",
       "      <td>I've been a Nintendo fan since the late 80's. ...</td>\n",
       "      <td>Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...</td>\n",
       "      <td>0.064259</td>\n",
       "      <td>0.456111</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.0</td>\n",
       "      <td>So the joycons works fine, they are original n...</td>\n",
       "      <td>Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...</td>\n",
       "      <td>0.105087</td>\n",
       "      <td>0.448241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1.0</td>\n",
       "      <td>Bought the Switch for Christmas 2017. It's Sep...</td>\n",
       "      <td>Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...</td>\n",
       "      <td>0.070055</td>\n",
       "      <td>0.329670</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1.0</td>\n",
       "      <td>I barely played my switch over the last year. ...</td>\n",
       "      <td>Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...</td>\n",
       "      <td>-0.052222</td>\n",
       "      <td>0.316667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.0</td>\n",
       "      <td>I bought this set of Joy-cons for a few reason...</td>\n",
       "      <td>Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...</td>\n",
       "      <td>0.111839</td>\n",
       "      <td>0.361653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1650 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   rating                                            content  \\\n",
       "0     1.0  I purchased two of these cards almost exactly ...   \n",
       "1     5.0  Amazon wants to know what I think about this i...   \n",
       "2     5.0  fast easy poor n broke due to fortnite i cant ...   \n",
       "3     5.0       El tiempo de llegada del código es inmediato   \n",
       "4     2.0  This should be a professional service, if you'...   \n",
       "..    ...                                                ...   \n",
       "5     2.0  I've been a Nintendo fan since the late 80's. ...   \n",
       "6     4.0  So the joycons works fine, they are original n...   \n",
       "7     1.0  Bought the Switch for Christmas 2017. It's Sep...   \n",
       "8     1.0  I barely played my switch over the last year. ...   \n",
       "9     4.0  I bought this set of Joy-cons for a few reason...   \n",
       "\n",
       "                                                title  polarity  subjectivity  \n",
       "0      $10 PlayStation Store Gift Card [Digital Code] -0.019792      0.230208  \n",
       "1      $10 PlayStation Store Gift Card [Digital Code]  0.100676      0.358117  \n",
       "2      $10 PlayStation Store Gift Card [Digital Code] -0.003496      0.566414  \n",
       "3      $10 PlayStation Store Gift Card [Digital Code]  0.000000      0.000000  \n",
       "4      $10 PlayStation Store Gift Card [Digital Code]  0.125000      0.462500  \n",
       "..                                                ...       ...           ...  \n",
       "5   Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...  0.064259      0.456111  \n",
       "6   Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...  0.105087      0.448241  \n",
       "7   Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...  0.070055      0.329670  \n",
       "8   Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ... -0.052222      0.316667  \n",
       "9   Nintendo Joy-Con (L)/(R) Fortnite Fleet Force ...  0.111839      0.361653  \n",
       "\n",
       "[1650 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd37d751",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
